#!/usr/bin/env python3
"""
ì£¼ì‹ ì§€ìˆ˜ ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“ˆ

ì£¼ìš” ê¸°ëŠ¥:
- ë‚˜ìŠ¤ë‹¥, S&P 500, ì½”ìŠ¤í”¼ ë“± ì£¼ìš” ì§€ìˆ˜ì˜ ì¼ë´‰ ë°ì´í„° ìˆ˜ì§‘
- ì „ê¸°ê°„ ë°ì´í„° ìˆ˜ì§‘ (ê°€ëŠ¥í•œ ëª¨ë“  ê¸°ê°„)
- ëª…ë ¹í–‰ ì¸ìˆ˜ë¥¼ í†µí•œ ìˆ˜ì§‘ ëŒ€ìƒ ì§€ì •
- ë°ì´í„° ìºì‹± ë° ì €ì¥

ì‚¬ìš©ë²•:
    python src/data_collector.py --indices NASDAQ SP500 KOSPI
    python src/data_collector.py --all
    python src/data_collector.py --help
"""

import argparse
import os
import sys
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import pandas as pd
import yfinance as yf
import requests
from pathlib import Path


class IndexDataCollector:
    """ì£¼ì‹ ì§€ìˆ˜ ë°ì´í„° ìˆ˜ì§‘ê¸°"""
    
    # ì§€ì›í•˜ëŠ” ì§€ìˆ˜ ëª©ë¡ (ì˜ˆìƒ ì‹œì‘ì¼ í¬í•¨)
    SUPPORTED_INDICES = {
        'NASDAQ': {'symbol': '^IXIC', 'expected_start': '1971-02-05'},      # ë‚˜ìŠ¤ë‹¥ ì¢…í•©ì§€ìˆ˜
        'SP500': {'symbol': '^GSPC', 'expected_start': '1957-03-04'},       # S&P 500 (ì‹¤ì œ 500ê°œ ê¸°ì—… ì‹œì‘)
        'DOW': {'symbol': '^DJI', 'expected_start': '1896-05-26'},          # ë‹¤ìš°ì¡´ìŠ¤
        'KOSPI': {'symbol': '^KS11', 'expected_start': '1980-01-04'},       # ì½”ìŠ¤í”¼ âš ï¸ ë°ì´í„° ëˆ„ë½
        'KOSDAQ': {'symbol': '^KQ11', 'expected_start': '1996-07-01'},      # ì½”ìŠ¤ë‹¥
        'NIKKEI': {'symbol': '^N225', 'expected_start': '1950-09-07'},      # ë‹ˆì¼€ì´ 225
        'FTSE': {'symbol': '^FTSE', 'expected_start': '1984-01-03'},        # FTSE 100
        'DAX': {'symbol': '^GDAXI', 'expected_start': '1959-12-31'},        # DAX
        'CAC': {'symbol': '^FCHI', 'expected_start': '1987-12-31'},         # CAC 40
        'HSI': {'symbol': '^HSI', 'expected_start': '1964-07-31'},          # í•­ì…ì§€ìˆ˜
    }
    
    def __init__(self, data_dir: str = "data"):
        """
        Args:
            data_dir: ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        self.cache_dir = self.data_dir / "cache"
        self.cache_dir.mkdir(exist_ok=True)
        
    def get_available_indices(self) -> Dict[str, dict]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì§€ìˆ˜ ëª©ë¡ ë°˜í™˜"""
        return self.SUPPORTED_INDICES.copy()
    
    def validate_indices(self, indices: List[str]) -> List[str]:
        """ì§€ìˆ˜ ì´ë¦„ ê²€ì¦ ë° ì •ê·œí™”"""
        valid_indices = []
        for index in indices:
            index_upper = index.upper()
            if index_upper in self.SUPPORTED_INDICES:
                valid_indices.append(index_upper)
            else:
                print(f"Warning: '{index}' is not a supported index. Skipping...")
                print(f"Supported indices: {', '.join(self.SUPPORTED_INDICES.keys())}")
        return valid_indices
    
    def collect_index_data(self, index_name: str, period: str = "max") -> Optional[pd.DataFrame]:
        """
        ì§€ìˆ˜ ë°ì´í„° ìˆ˜ì§‘
        
        Args:
            index_name: ì§€ìˆ˜ ì´ë¦„ (ì˜ˆ: 'NASDAQ', 'SP500')
            period: ë°ì´í„° ìˆ˜ì§‘ ê¸°ê°„ ('max' for ì „ì²´ ê¸°ê°„)
            
        Returns:
            DataFrame with OHLCV data or None if failed
        """
        if index_name not in self.SUPPORTED_INDICES:
            print(f"Error: Unsupported index '{index_name}'")
            return None
            
        symbol = self.SUPPORTED_INDICES[index_name]['symbol']
        expected_start = pd.to_datetime(self.SUPPORTED_INDICES[index_name]['expected_start'])
        
        try:
            print(f"Collecting data for {index_name} ({symbol})...")
            
            # yfinanceë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ìˆ˜ì§‘
            ticker = yf.Ticker(symbol)
            data = ticker.history(period=period, interval="1d")
            
            if data.empty:
                print(f"Warning: No data found for {index_name}")
                return None
            
            # S&P 500ì˜ ê²½ìš° 1957ë…„ ì´í›„ ë°ì´í„°ë§Œ í•„í„°ë§
            if index_name == 'SP500':
                original_count = len(data)
                # íƒ€ì„ì¡´ì„ ê³ ë ¤í•œ í•„í„°ë§
                if data.index.tz is not None:
                    expected_start = expected_start.tz_localize(data.index.tz)
                data = data[data.index >= expected_start]
                filtered_count = len(data)
                print(f"  ğŸ“… S&P 500: 1957ë…„ ì´í›„ ë°ì´í„°ë¡œ í•„í„°ë§ ({original_count} â†’ {filtered_count} days)")
                
            # ì»¬ëŸ¼ëª… ì •ë¦¬
            data.index.name = 'Date'
            data = data.round(2)  # ì†Œìˆ˜ì  2ìë¦¬ë¡œ ë°˜ì˜¬ë¦¼
            
            # ë°ì´í„° ì™„ì„±ë„ ê²€ì¦
            expected_start = pd.to_datetime(self.SUPPORTED_INDICES[index_name]['expected_start']).date()
            actual_start = data.index[0].date()
            actual_end = data.index[-1].date()
            
            print(f"âœ“ {index_name}: {len(data)} days of data collected")
            print(f"  Period: {actual_start} to {actual_end}")
            
            # ëˆ„ë½ ë°ì´í„° ê²½ê³ 
            if actual_start > expected_start:
                missing_days = (actual_start - expected_start).days
                missing_years = round(missing_days / 365.25, 1)
                print(f"  âš ï¸  Warning: {missing_years} years of data missing from expected start ({expected_start})")
                if index_name in ['KOSPI', 'KOSDAQ']:
                    print(f"     ğŸ’¡ Tip: Use 'python src/korea_data_collector.py --indices {index_name}' for better Korea data")
            
            return data
            
        except Exception as e:
            print(f"Error collecting data for {index_name}: {str(e)}")
            return None
    
    def save_data(self, data: pd.DataFrame, index_name: str) -> str:
        """
        ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            data: ì €ì¥í•  ë°ì´í„°
            index_name: ì§€ìˆ˜ ì´ë¦„
            
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        filename = f"{index_name}_data.csv"
        filepath = self.data_dir / filename
        
        # ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•© (ìˆëŠ” ê²½ìš°)
        if filepath.exists():
            try:
                existing_data = pd.read_csv(filepath, index_col=0, parse_dates=True)
                # ì¤‘ë³µ ì œê±°í•˜ê³  ë³‘í•©
                combined_data = pd.concat([existing_data, data])
                combined_data = combined_data[~combined_data.index.duplicated(keep='last')]
                combined_data = combined_data.sort_index()
                data = combined_data
                print(f"  Updated existing data for {index_name}")
            except Exception as e:
                print(f"  Warning: Could not merge with existing data: {e}")
        
        data.to_csv(filepath)
        print(f"  Saved to: {filepath}")
        return str(filepath)
    
    def collect_multiple_indices(self, indices: List[str]) -> Dict[str, str]:
        """
        ì—¬ëŸ¬ ì§€ìˆ˜ì˜ ë°ì´í„°ë¥¼ ì¼ê´„ ìˆ˜ì§‘
        
        Args:
            indices: ìˆ˜ì§‘í•  ì§€ìˆ˜ ì´ë¦„ ëª©ë¡
            
        Returns:
            {index_name: filepath} ë”•ì…”ë„ˆë¦¬
        """
        results = {}
        valid_indices = self.validate_indices(indices)
        
        if not valid_indices:
            print("No valid indices to collect.")
            return results
        
        print(f"\nStarting data collection for {len(valid_indices)} indices...")
        print("=" * 60)
        
        for i, index_name in enumerate(valid_indices, 1):
            print(f"\n[{i}/{len(valid_indices)}] Processing {index_name}...")
            
            data = self.collect_index_data(index_name)
            if data is not None:
                filepath = self.save_data(data, index_name)
                results[index_name] = filepath
            else:
                print(f"  Failed to collect data for {index_name}")
        
        return results
    
    def get_summary(self) -> pd.DataFrame:
        """ì €ì¥ëœ ë°ì´í„° íŒŒì¼ë“¤ì˜ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        summary_data = []
        
        for index_name, info in self.SUPPORTED_INDICES.items():
            filepath = self.data_dir / f"{index_name}_data.csv"
            if filepath.exists():
                try:
                    data = pd.read_csv(filepath, index_col=0, parse_dates=True)
                    expected_start = pd.to_datetime(info['expected_start']).date()
                    actual_start = data.index.min().date()
                    
                    # ëˆ„ë½ ì—°ìˆ˜ ê³„ì‚°
                    missing_years = 0
                    if actual_start > expected_start:
                        missing_days = (actual_start - expected_start).days
                        missing_years = round(missing_days / 365.25, 1)
                    
                    summary_data.append({
                        'Index': index_name,
                        'Symbol': info['symbol'],
                        'Records': len(data),
                        'Start_Date': actual_start,
                        'End_Date': data.index.max().date(),
                        'Expected_Start': expected_start,
                        'Missing_Years': missing_years,
                        'File_Size_MB': round(filepath.stat().st_size / 1024 / 1024, 2)
                    })
                except Exception as e:
                    print(f"Error reading {filepath}: {e}")
        
        return pd.DataFrame(summary_data)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="ì£¼ì‹ ì§€ìˆ˜ ë°ì´í„° ìˆ˜ì§‘ê¸°",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  %(prog)s --indices NASDAQ SP500 KOSPI    # íŠ¹ì • ì§€ìˆ˜ë“¤ ìˆ˜ì§‘
  %(prog)s --all                          # ëª¨ë“  ì§€ìˆ˜ ìˆ˜ì§‘
  %(prog)s --list                         # ì§€ì›í•˜ëŠ” ì§€ìˆ˜ ëª©ë¡ ì¶œë ¥
  %(prog)s --summary                      # ì €ì¥ëœ ë°ì´í„° ìš”ì•½ ì¶œë ¥
        """
    )
    
    parser.add_argument(
        '--indices', 
        nargs='+', 
        help='ìˆ˜ì§‘í•  ì§€ìˆ˜ ì´ë¦„ë“¤ (ì˜ˆ: NASDAQ SP500 KOSPI)'
    )
    
    parser.add_argument(
        '--all', 
        action='store_true',
        help='ëª¨ë“  ì§€ì›í•˜ëŠ” ì§€ìˆ˜ ë°ì´í„° ìˆ˜ì§‘'
    )
    
    parser.add_argument(
        '--list', 
        action='store_true',
        help='ì§€ì›í•˜ëŠ” ì§€ìˆ˜ ëª©ë¡ ì¶œë ¥'
    )
    
    parser.add_argument(
        '--summary', 
        action='store_true',
        help='ì €ì¥ëœ ë°ì´í„° íŒŒì¼ ìš”ì•½ ì •ë³´ ì¶œë ¥'
    )
    
    parser.add_argument(
        '--data-dir', 
        default='data',
        help='ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: data)'
    )
    
    args = parser.parse_args()
    
    # ë°ì´í„° ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
    collector = IndexDataCollector(args.data_dir)
    
    # ì§€ì›í•˜ëŠ” ì§€ìˆ˜ ëª©ë¡ ì¶œë ¥
    if args.list:
        print("ì§€ì›í•˜ëŠ” ì£¼ì‹ ì§€ìˆ˜:")
        print("=" * 60)
        for name, info in collector.get_available_indices().items():
            missing_note = " âš ï¸ ë°ì´í„° ëˆ„ë½" if name in ['KOSPI', 'KOSDAQ'] else ""
            print(f"  {name:<10} : {info['symbol']} (ì˜ˆìƒì‹œì‘: {info['expected_start']}){missing_note}")
        return
    
    # ì €ì¥ëœ ë°ì´í„° ìš”ì•½ ì¶œë ¥
    if args.summary:
        print("ì €ì¥ëœ ë°ì´í„° ìš”ì•½:")
        print("=" * 60)
        summary = collector.get_summary()
        if not summary.empty:
            print(summary.to_string(index=False))
        else:
            print("ì €ì¥ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰
    if args.all:
        # ëª¨ë“  ì§€ìˆ˜ ìˆ˜ì§‘
        indices = list(collector.get_available_indices().keys())
        print(f"ëª¨ë“  ì§€ìˆ˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤: {', '.join(indices)}")
    elif args.indices:
        # ì§€ì •ëœ ì§€ìˆ˜ë“¤ ìˆ˜ì§‘
        indices = args.indices
    else:
        # ì¸ìˆ˜ê°€ ì—†ìœ¼ë©´ ë„ì›€ë§ ì¶œë ¥
        parser.print_help()
        return
    
    # ìˆ˜ì§‘ ì‹œì‘
    start_time = datetime.now()
    results = collector.collect_multiple_indices(indices)
    end_time = datetime.now()
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ìˆ˜ì§‘ ì™„ë£Œ!")
    print(f"ì†Œìš” ì‹œê°„: {end_time - start_time}")
    print(f"ì„±ê³µ: {len(results)}/{len(indices)} ì§€ìˆ˜")
    
    if results:
        print("\nì €ì¥ëœ íŒŒì¼:")
        for index_name, filepath in results.items():
            print(f"  {index_name}: {filepath}")
    
    # ìµœì¢… ìš”ì•½ ì¶œë ¥
    summary = collector.get_summary()
    if not summary.empty:
        print(f"\nì „ì²´ ë°ì´í„° í˜„í™©:")
        print(f"ì´ {len(summary)}ê°œ ì§€ìˆ˜, {summary['Records'].sum():,}ê°œ ë ˆì½”ë“œ")


if __name__ == "__main__":
    main()